{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Apache Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## append.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/append.py\n"
     ]
    }
   ],
   "source": [
    "%%file scripts/append.py\n",
    "\n",
    "import argparse\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "# define a function for transforming the data \n",
    "class AppendDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        yield element + \" - Hello World!\"\n",
    "        \n",
    "# set up pipeline parameters \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input', dest='input',\n",
    "                    default='gs://dataflow-samples/shakespeare/kinglear.txt')\n",
    "parser.add_argument('--output', dest='output',\n",
    "                    default='gs://dsp_model_store_00/shakespeare/kinglear.txt')\n",
    "known_args, pipeline_args = parser.parse_known_args()\n",
    "pipeline_options = PipelineOptions(pipeline_args)\n",
    "\n",
    "# define the pipeline steps \n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "lines = p | 'read' >> ReadFromText(known_args.input)\n",
    "appended = lines | 'append' >> beam.ParDo(AppendDoFn())\n",
    "appended | 'write' >> WriteToText(known_args.output)\n",
    "\n",
    "# run the pipeline \n",
    "result = p.run()\n",
    "result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# run locally\n",
    "python3 scripts/append.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# run managed\n",
    "python3 scripts/append.py \\\n",
    "    --runner DataflowRunner \\\n",
    "    --project $GOOGLE_PROJECT_ID \\\n",
    "    --region us-central1 \\\n",
    "    --temp_location gs://dsp_model_store_00/tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Batch Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataflow_read.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/dataflow_read.py\n"
     ]
    }
   ],
   "source": [
    "%%file scripts/dataflow_read.py\n",
    "\n",
    "import argparse\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "known_args, pipeline_args = parser.parse_known_args()\n",
    "pipeline_options = PipelineOptions(pipeline_args)\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        `bigquery-public-data.samples.natality`\n",
    "    ORDER BY\n",
    "        RAND()\n",
    "    LIMIT\n",
    "        5\n",
    "\"\"\"\n",
    "\n",
    "# define the pipeline steps\n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "data = p | 'Read from BigQuery' >> beam.io.Read(\n",
    "    beam.io.BigQuerySource(query=query, use_standard_sql=True)\n",
    ")\n",
    "scored = data | 'Print' >> beam.Map(print)\n",
    "\n",
    "# run the pipeline\n",
    "result = p.run()\n",
    "result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# run locally\n",
    "python3 scripts/dataflow_read.py --project $GOOGLE_PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>plurality</th>\n",
       "      <th>apgar_5min</th>\n",
       "      <th>mother_age</th>\n",
       "      <th>father_age</th>\n",
       "      <th>gestation_weeks</th>\n",
       "      <th>ever_born</th>\n",
       "      <th>mother_married</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.251004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.436599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.311835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1979</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17</td>\n",
       "      <td>99</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.876218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  plurality  apgar_5min  mother_age  father_age  gestation_weeks  \\\n",
       "0  1984        1.0        10.0          18          22             38.0   \n",
       "1  1980        1.0         9.0          16          17             99.0   \n",
       "2  1983        1.0         8.0          21          26             38.0   \n",
       "3  1985        1.0         9.0          30          30             39.0   \n",
       "4  1979        1.0        10.0          17          99             39.0   \n",
       "\n",
       "   ever_born  mother_married    weight  \n",
       "0        1.0               1  7.251004  \n",
       "1        1.0               0  5.436599  \n",
       "2        2.0               1  6.311835  \n",
       "3        3.0               1  8.000575  \n",
       "4        1.0               0  6.876218  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "sql = \"\"\"\n",
    "    SELECT \n",
    "        year,\n",
    "        plurality, \n",
    "        apgar_5min,\n",
    "        mother_age, \n",
    "        father_age,    \n",
    "        gestation_weeks, \n",
    "        ever_born,\n",
    "        CASE WHEN mother_married = true THEN 1 ELSE 0 END AS mother_married,\n",
    "        weight_pounds AS weight\n",
    "    FROM\n",
    "        `bigquery-public-data.samples.natality`\n",
    "    ORDER BY\n",
    "        RAND()\n",
    "    LIMIT\n",
    "        10000\n",
    "\"\"\"\n",
    "\n",
    "natality_df = client.query(sql).to_dataframe().fillna(0)\n",
    "natality_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from google.cloud import storage\n",
    "\n",
    "# fit and pickle a model \n",
    "model = LinearRegression()\n",
    "model.fit(natality_df.drop(columns='weight'), natality_df['weight'])\n",
    "joblib.dump(model, 'natality.pkl')\n",
    "\n",
    "# Save to GCS\n",
    "bucket = storage.Client().get_bucket('dsp_model_store_00')\n",
    "blob = bucket.blob('natality/sklearn-linear')\n",
    "blob.upload_from_filename('natality.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from google.cloud import storage\n",
    "\n",
    "bucket = storage.Client().get_bucket('dsp_model_store_00')\n",
    "blob = bucket.get_blob('natality/sklearn-linear')\n",
    "blob.download_to_filename('sklearn-linear')\n",
    "model = joblib.load('sklearn-linear')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BigQuery-Datastore Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/apply.py\n"
     ]
    }
   ],
   "source": [
    "%%file scripts/apply.py\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import apache_beam as beam\n",
    "\n",
    "from google.cloud import storage\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.io.gcp.bigquery_tools import parse_table_schema_from_json\n",
    "from apache_beam.io.gcp.datastore.v1new.types import Key\n",
    "from apache_beam.io.gcp.datastore.v1new.types import Entity\n",
    "from apache_beam.io.gcp.datastore.v1new.datastoreio import WriteToDatastore\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        year,\n",
    "        plurality, \n",
    "        apgar_5min,\n",
    "        mother_age, \n",
    "        father_age,    \n",
    "        gestation_weeks, \n",
    "        ever_born,\n",
    "        CASE WHEN mother_married = true THEN 1 \n",
    "             ELSE 0\n",
    "        END AS mother_married,\n",
    "        weight_pounds AS weight,\n",
    "        CURRENT_TIMESTAMP AS time,\n",
    "        GENERATE_UUID() AS guid\n",
    "    FROM\n",
    "        `bigquery-public-data.samples.natality`\n",
    "    LIMIT\n",
    "        100\n",
    "\"\"\"\n",
    "\n",
    "class ApplyDoFn(beam.DoFn):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "     \n",
    "    def process(self, element):\n",
    "        if self.model is None:\n",
    "            bucket = storage.Client().get_bucket('dsp_model_store_00')\n",
    "            blob = bucket.get_blob('natality/sklearn-linear')\n",
    "            blob.download_to_filename('sklearn-linear')\n",
    "            self.model = joblib.load('sklearn-linear')\n",
    "        \n",
    "        new_x = pd.DataFrame.from_dict(element, orient=\"index\").T.fillna(0)   \n",
    "        weight = self.model.predict(new_x.iloc[:, :8])[0]\n",
    "        yield {'guid': element['guid'],\n",
    "               'weight': weight,\n",
    "               'time': str(element['time'])}\n",
    "\n",
    "schema = parse_table_schema_from_json(json.dumps({\n",
    "    'fields': [{'name': 'guid', 'type': 'STRING'},\n",
    "               {'name': 'weight', 'type': 'FLOAT64'},\n",
    "               {'name': 'time', 'type': 'STRING'}]\n",
    "}))\n",
    "\n",
    "class CreateEntityDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        key = Key(['natality-guid', element['guid']])\n",
    "        entity = Entity(key)\n",
    "        entity.set_properties({\n",
    "            'weight': element['weight'],\n",
    "            'time': element['time']\n",
    "        })\n",
    "        yield entity\n",
    "\n",
    "# set up pipeline options\n",
    "parser = argparse.ArgumentParser()\n",
    "known_args, pipeline_args = parser.parse_known_args()\n",
    "pipeline_options = PipelineOptions(pipeline_args)\n",
    "project = pipeline_options.view_as(GoogleCloudOptions).project\n",
    "\n",
    "# define the pipeline steps\n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "data = p | 'Read from BigQuery' >> beam.io.ReadFromBigQuery(\n",
    "    query=query, \n",
    "    use_standard_sql=True\n",
    ")\n",
    "scored = data | 'Apply Model' >> beam.ParDo(ApplyDoFn())\n",
    "scored | 'Save to BigQuery' >> beam.io.WriteToBigQuery(\n",
    "    table='weight_preds',\n",
    "    dataset='dsp_demo', \n",
    "    schema=schema,\n",
    "    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "    write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    ")\n",
    "(scored\n",
    " | 'Create Entities' >> beam.ParDo(CreateEntityDoFn())\n",
    " | 'Save to Datastore' >> WriteToDatastore(project))\n",
    "\n",
    "# run the pipeline\n",
    "result = p.run()\n",
    "result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# running locally\n",
    "python3 scripts/apply.py \\\n",
    "    --project $GOOGLE_PROJECT_ID \\\n",
    "    --temp_location gs://dsp_model_store_00/tmp/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# running on GCP\n",
    "echo 'google-cloud-storage==1.29.0' > reqs.txt \\\n",
    "&& python3 scripts/apply.py \\\n",
    "    --runner DataflowRunner \\\n",
    "    --project $GOOGLE_PROJECT_ID \\\n",
    "    --region us-central1 \\\n",
    "    --temp_location gs://dsp_model_store_00/tmp/ \\\n",
    "    --requirements_file reqs.txt \\\n",
    "    --max_num_workers 5 \\\n",
    "    --save_main_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datastore_read.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/datastore_read.py\n"
     ]
    }
   ],
   "source": [
    "%%file scripts/datastore_read.py\n",
    "\n",
    "import argparse\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.io.gcp.datastore.v1new.datastoreio import ReadFromDatastore\n",
    "from apache_beam.io.gcp.datastore.v1new.types import Query\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "known_args, pipeline_args = parser.parse_known_args(None)\n",
    "pipeline_options = PipelineOptions(pipeline_args)\n",
    "project = pipeline_options.view_as(GoogleCloudOptions).project\n",
    "\n",
    "# define the pipeline steps\n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "data = p | 'Read from Datastore' >> ReadFromDatastore(\n",
    "    query=Query('natality-guid', project, limit=5)\n",
    ")\n",
    "scored = data | 'Print' >> beam.Map(print)\n",
    "\n",
    "# run the pipeline\n",
    "result = p.run()\n",
    "result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# run locally\n",
    "python3 scripts/datastore_read.py --project $GOOGLE_PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Entity('natality-guid', '008b402d-32ff-4b14-aba0-3a4b2d712363') {'time': '2020-09-28 10:03:14.660679+00:00', 'weight': 7.449097035666264}>\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import datastore\n",
    "client = datastore.Client()\n",
    "query = client.query(kind='natality-guid')\n",
    "\n",
    "query_iter = query.fetch()\n",
    "for entity in query_iter:\n",
    "    print(entity)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
