{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")\n",
    " \n",
    "data = {'hello': 'world', 'time': time.time()}\n",
    "producer.send('dsp', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'dsp',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    " \n",
    "for x in consumer:\n",
    "    print(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "external_ip = os.environ['KAFKA_BOOTSTRAP_SERVER']\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{external_ip}:9092\")\n",
    "    .option(\"subscribe\", \"dsp\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .withColumn(\"value_deserialized\", \n",
    "                udf(lambda x: json.loads(x.decode('utf-8')))(\"value\"))\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "\n",
    "external_ip = os.environ['KAFKA_BOOTSTRAP_SERVER']\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[f'{external_ip}:9092'],\n",
    "    value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
    ")\n",
    "\n",
    "data = {'G1': 1, 'G2': 0, 'G3': 0, 'G4': 0, 'G5': 0, \n",
    "        'G6': 0, 'G7': 0, 'G8': 0, 'G9': 0, 'G10': 0, \n",
    "        'User_ID': str(uuid.uuid1())}\n",
    "result = producer.send('dsp', data)\n",
    "result.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StringType \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "external_ip = os.environ['KAFKA_BOOTSTRAP_SERVER']\n",
    "\n",
    "# build a logsitic regression model \n",
    "games_df = pd.read_csv(\"https://github.com/bgweber/Twitch/raw/master/Recommendations/games-expand.csv\")\n",
    "model = LogisticRegression()\n",
    "model.fit(games_df.drop(columns='label'), games_df['label'])\n",
    "\n",
    "# read from Kafka \n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{external_ip}:9092\")\n",
    "    .option(\"subscribe\", \"dsp\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# define the UDF for scoring users \n",
    "def score(value):\n",
    "    new_row = json.loads(value)\n",
    "    new_x = pd.DataFrame.from_dict(data=new_row, orient=\"index\").T        \n",
    "    pred = model.predict_proba(new_x.drop(columns='User_ID'))[0][1]\n",
    "    result = {'User_ID': new_row['User_ID'], 'pred': pred}\n",
    "    return json.dumps(result)\n",
    "  \n",
    "# select the value field and apply the UDF     \n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "score_udf = udf(score, StringType())    \n",
    "df = df.select(score_udf(\"value\").alias(\"value\"))\n",
    "\n",
    "# write results to Kafka \n",
    "query = (\n",
    "    df.writeStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{external_ip}:9092\")\n",
    "    .option(\"topic\", \"preds\")\n",
    "    .option(\"checkpointLocation\", \"/temp\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "\n",
    "external_ip = os.getenv('KAFKA_BOOTSTRAP_SERVER')\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'preds',\n",
    "    bootstrap_servers=[f'{external_ip}:9092'],\n",
    "    value_deserializer=lambda x: json.loads(x)\n",
    ")\n",
    "\n",
    "for x in consumer:\n",
    "    print(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 Dataflow Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PubSub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubSub Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "subscriber = pubsub_v1.SubscriberClient()\n",
    "subscription_path = subscriber.subscription_path(\n",
    "    project=os.environ['GOOGLE_PROJECT_ID'], \n",
    "    subscription=\"dsp\"\n",
    ")\n",
    "\n",
    "def callback(message):\n",
    "    print(message.data)\n",
    "    message.ack()\n",
    "\n",
    "future = subscriber.subscribe(\n",
    "    subscription=subscription_path, \n",
    "    callback=callback\n",
    ")\n",
    "\n",
    "try:\n",
    "    future.result()\n",
    "except KeyboardInterrupt:\n",
    "    future.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubSub Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(\n",
    "    project=os.environ['GOOGLE_PROJECT_ID'], \n",
    "    topic=\"natality\"\n",
    ")\n",
    "\n",
    "data = \"Hello World!\".encode('utf-8')\n",
    "publisher.publish(topic=topic_path, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/stream.py\n"
     ]
    }
   ],
   "source": [
    "%%file scripts/stream.py\n",
    "\n",
    "import argparse\n",
    "import apache_beam as beam\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "\n",
    "# define a function for transforming the data \n",
    "class AppendDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        print(\"Hello World! - \" + element.decode('utf-8'))\n",
    "        \n",
    "# set up pipeline options\n",
    "parser = argparse.ArgumentParser()\n",
    "known_args, pipeline_args = parser.parse_known_args()\n",
    "pipeline_options = PipelineOptions(pipeline_args)\n",
    "project = pipeline_options.view_as(GoogleCloudOptions).project\n",
    "\n",
    "# define the topics \n",
    "topic = \"projects/{project}/topics/{topic}\"\n",
    "topic = topic.format(project=project, topic=\"natality\")\n",
    "\n",
    "# define the pipeline steps \n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "(p\n",
    " | 'Read PubSub' >> beam.io.ReadFromPubSub(topic=topic)\n",
    " | 'Append' >> beam.ParDo(AppendDoFn()))\n",
    "\n",
    "# run the pipeline \n",
    "result = p.run()\n",
    "result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# running locally\n",
    "python3 scripts/stream.py \\\n",
    "    --streaming \\\n",
    "    --project $GOOGLE_PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natality Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/natality.py\n"
     ]
    }
   ],
   "source": [
    "%%file scripts/natality.py\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import apache_beam as beam\n",
    "\n",
    "from google.cloud import storage\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.io.gcp.datastore.v1new.types import Key\n",
    "from apache_beam.io.gcp.datastore.v1new.types import Entity\n",
    "from apache_beam.io.gcp.datastore.v1new.datastoreio import WriteToDatastore\n",
    "\n",
    "class ApplyDoFn(beam.DoFn):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "     \n",
    "    def process(self, element):\n",
    "        if self.model is None:\n",
    "            bucket = storage.Client().get_bucket('dsp_model_store_00')\n",
    "            blob = bucket.get_blob('natality/sklearn-linear')\n",
    "            blob.download_to_filename('sklearn-linear')\n",
    "            self.model = joblib.load('sklearn-linear')\n",
    "        \n",
    "        element = json.loads(element.decode('utf-8'))\n",
    "        new_x = pd.DataFrame.from_dict(element, orient=\"index\").T.fillna(0)   \n",
    "        weight = self.model.predict(new_x.iloc[:, :8])[0]\n",
    "        yield {'guid': element['guid'],\n",
    "               'weight': weight,\n",
    "               'time': str(element['time'])}\n",
    "        \n",
    "class CreateEntityDoFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        key = Key(['natality-guid', element['guid']])\n",
    "        entity = Entity(key)\n",
    "        entity.set_properties({\n",
    "            'weight': element['weight'],\n",
    "            'time': element['time']\n",
    "        })\n",
    "        yield entity\n",
    "        \n",
    "# set up pipeline options\n",
    "parser = argparse.ArgumentParser()\n",
    "known_args, pipeline_args = parser.parse_known_args()\n",
    "pipeline_options = PipelineOptions(pipeline_args)\n",
    "project = pipeline_options.view_as(GoogleCloudOptions).project\n",
    "\n",
    "# define the topics \n",
    "topic = \"projects/{project}/topics/{topic}\"\n",
    "topic = topic.format(project=project, topic=\"natality\")\n",
    "\n",
    "# define the pipeline steps\n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "(p\n",
    " | 'Read PubSub' >> beam.io.ReadFromPubSub(topic=topic)\n",
    " | 'Apply Model' >> beam.ParDo(ApplyDoFn())\n",
    " | 'Create Entities' >> beam.ParDo(CreateEntityDoFn())\n",
    " | 'Save to Datastore' >> WriteToDatastore(project)\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "result = p.run()\n",
    "result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# running locally\n",
    "python3 scripts/natality.py \\\n",
    "    --streaming \\\n",
    "    --project $GOOGLE_PROJECT_ID"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# running on GCP\n",
    "echo 'google-cloud-storage==1.29.0' > reqs.txt \\\n",
    "&& python3 scripts/natality.py \\\n",
    "    --streaming \\\n",
    "    --runner DataflowRunner \\\n",
    "    --project $GOOGLE_PROJECT_ID \\\n",
    "    --region us-central1 \\\n",
    "    --temp_location gs://dsp_model_store_00/tmp/ \\\n",
    "    --requirements_file reqs.txt \\\n",
    "    --max_num_workers 5 \\\n",
    "    --save_main_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(\n",
    "    project=os.environ['GOOGLE_PROJECT_ID'], \n",
    "    topic=\"natality\"\n",
    ")\n",
    "\n",
    "data = json.dumps({\n",
    "    'year': 2001, \n",
    "    'plurality': 1, \n",
    "    'apgar_5min': 99, \n",
    "    'mother_age': 33, \n",
    "    'father_age': 40, \n",
    "    'gestation_weeks': 38, \n",
    "    'ever_born': 8, \n",
    "    'mother_married': 1,\n",
    "    'weight': 6.8122838958, \n",
    "    'time': str(time.time()), \n",
    "    'guid': 'b281c5e8-85b2-4cbd-a2d8-e501ca816363'\n",
    "}).encode('utf-8') \n",
    "\n",
    "publisher.publish(topic=topic_path, data=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
